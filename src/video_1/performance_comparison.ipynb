{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparison\n",
    "\n",
    "In this notebook, we will demonstrate briefly how the `COPY FROM` technique to ingest data is orders\n",
    "of magnitude faster than Cypher's `CREATE` statement to ingest data. The difference in\n",
    "performance is due to the fact that we have a separate query processing pipeline specialized for\n",
    "`COPY` that assumes a large amount of data is being inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kuzu\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some mock data\n",
    "\n",
    "You can generate some mock data for this test case using the following cell (simply uncomment it and run it).\n",
    "The data is written to `data/person_profiles.csv` and contains some mock data of persons and their metadata.\n",
    "Just like in the real world, the data is a combination of integers, floats and short/long-form strings.\n",
    "\n",
    "To run the cells below, install the `faker` and `polars` libraries within your Python environment:\n",
    "\n",
    "```bash\n",
    "uv pip install faker polars\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ester',\n",
       " 'Aleck',\n",
       " 'Birtha',\n",
       " 'Chantel',\n",
       " 'Sydnie',\n",
       " 'Lisandro',\n",
       " 'Caitlyn',\n",
       " 'Normand',\n",
       " 'Stephania',\n",
       " 'Berton']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-collect the entire list of first names from the Faker library\n",
    "import random\n",
    "from faker.providers.person.en import Provider\n",
    "\n",
    "SEED = 37\n",
    "random.seed(SEED)\n",
    "\n",
    "first_names = list(set(Provider.first_names))\n",
    "random.shuffle(first_names)\n",
    "first_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Generated 5000 synthetic person profiles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# uv pip install faker polars\n",
    "from faker import Faker\n",
    "import polars as pl\n",
    "\n",
    "Faker.seed(SEED)\n",
    "fake = Faker()\n",
    "\n",
    "NUM_RECORDS = 5000\n",
    "OUTPUT_PATH = \"data/person_profiles.csv\"\n",
    "\n",
    "def generate_person_profiles(num: int) -> None:\n",
    "    profiles = []\n",
    "    for i in range(1, NUM_RECORDS + 1):\n",
    "        profile = dict()\n",
    "        profile[\"id\"] = i\n",
    "        profile[\"name\"] = first_names[i]\n",
    "        profile[\"age\"] = fake.random_int(min=18, max=75)\n",
    "        profile[\"net_worth\"] = fake.pyfloat(positive=True, min_value=10245, max_value=100_321_251)\n",
    "        profile[\"email\"] = f\"{fake.domain_word()}@{fake.free_email_domain()}\"\n",
    "        profile[\"address\"] = fake.address().replace(\"\\n\", \", \")\n",
    "        profile[\"phone\"] = fake.phone_number()\n",
    "        profile[\"comments\"] = fake.text(max_nb_chars=200)\n",
    "        profiles.append(profile)\n",
    "    print(f\"---\\nGenerated {num} synthetic person profiles.\\n\")\n",
    "    # Output to CSV file using Polars\n",
    "    df = pl.DataFrame(profiles)\n",
    "    df.write_csv(OUTPUT_PATH, separator=\"|\")\n",
    "\n",
    "generate_person_profiles(NUM_RECORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a K첫zu database\n",
    "\n",
    "Just as in the other notebook, we will create a K첫zu database and start a connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"./db_large\"\n",
    "shutil.rmtree(DB_NAME, ignore_errors=True)\n",
    "db = kuzu.Database(DB_NAME)\n",
    "conn = kuzu.Connection(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a K첫zu table\n",
    "\n",
    "A node table is created with the below schema that matches the columns in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_table(name: str) -> None:\n",
    "    conn.execute(\n",
    "        f\"\"\"\n",
    "        CREATE NODE TABLE {name} (\n",
    "            id STRING,\n",
    "            name STRING,\n",
    "            age INT64,\n",
    "            net_worth DOUBLE,\n",
    "            email STRING,\n",
    "            address STRING,\n",
    "            phone STRING,\n",
    "            comments STRING,\n",
    "            PRIMARY KEY (id)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "create_node_table(\"Person\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV data\n",
    "\n",
    "The CSV data is read into a list of dicts in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_csv(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\"|\")\n",
    "        for line in reader:\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "records = read_csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Use `CREATE` to ingest the nodes\n",
    "\n",
    "The most naive way to ingest nodes into K첫zu via Cypher is using the `CREATE` clause. In this case,\n",
    "we iterate through the list of records and create a node for each record.\n",
    "\n",
    " The `CREATE` clause will only add a node if a node with the same primary key value does not already exist - if it exists, there will be a Runtime error. Another similar clause that does this while overwriting existing nodes with the same primary key value is the `MERGE` clause, which will add a node if it does not exist, or update the node if it does exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 s, sys: 876 ms, total: 2.55 s\n",
      "Wall time: 1.77 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<kuzu.query_result.QueryResult at 0x10aa67b60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "conn.execute(\"BEGIN TRANSACTION\")\n",
    "for record in records:\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE (person:Person {id: $id})\n",
    "        SET person.name = $name,\n",
    "            person.age = $age,\n",
    "            person.net_worth = $net_worth,\n",
    "            person.email = $email,\n",
    "            person.address = $address,\n",
    "            person.phone = $phone,\n",
    "            person.comments = $comments\n",
    "        \"\"\",\n",
    "        parameters={\n",
    "            \"id\": record[\"id\"],\n",
    "            \"name\": record[\"name\"],\n",
    "            \"age\": int(record[\"age\"]),\n",
    "            \"net_worth\": float(record[\"net_worth\"]),\n",
    "            \"email\": record[\"email\"],\n",
    "            \"address\": record[\"address\"],\n",
    "            \"phone\": record[\"phone\"],\n",
    "            \"comments\": record[\"comments\"],\n",
    "        }\n",
    "    )\n",
    "conn.execute(\"COMMIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the time taken for the above cell to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Use `COPY FROM` to ingest the nodes\n",
    "\n",
    "The next step is to perform the same task, but using the `COPY FROM` statement. As can be seen from the timing numbers below, it's much, much faster than using individual `CREATE` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the table and recreate it\n",
    "conn.execute(\"DROP TABLE Person\")\n",
    "create_node_table(\"Person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 ms, sys: 58.8 ms, total: 81.5 ms\n",
      "Wall time: 31.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conn.execute(\"COPY Person FROM 'data/person_profiles.csv' (header = true, delim = '|', parallel = false)\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
